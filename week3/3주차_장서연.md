# week3 transformer architecture

# Transformer

![jaehee1.png](https://imghub.insilicogen.com/media/photos/encoder_decoder.png)

RNN을 사용하지 않지만 기존의 seq2seq의 인코더-디코더 구조 유지

인코더와 디코더 N개로 구성, 논문에서는 6개 사용

## positional encoding

transformer는 단어 입력을 순차적으로 받지 않아 단어의 위치 정보를 다른 방식으로 알려주어야 함.

→ **각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용** (positional encoding)

![transformer5_final_final.png](https://wikidocs.net/images/page/31379/transformer4_final_final_final.PNG)

임베딩 벡터가 인코더의 입력으로 사용되기 전에 positional encoding 값이 더해짐.

→ 같은 단어라도 문장 내의 위치에 따라서 입력으로 들어가는 임베딩 벡터의 값이 달라짐.

$$
PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})
$$

$$
PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_{model}})
$$

$pos$: 임베딩 벡터의 위치 / $i$: 임베딩 벡터 내의 차원의 인덱스 / $d_model$: transformer의 모든 층의 출력 차원(논문에서는 512)

![transformer7.png](https://wikidocs.net/images/page/31379/transformer7.PNG)

## Attention

![쿼리.png](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)

주어진 query에 대해서 모든 key와의 유사도를 구함

→ 이 유사도를 가중치로 하여 key와 mapping되어 있는 각각의 value에 반영

→ 유사도가 반영된 value를 모두 가중합하여 return

attention을 자기 자신에게 수행 ⇒ **‘self attention’**

![attention.png](https://wikidocs.net/images/page/31379/attention.PNG)

![transformer_attention_overview.png](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)

- self attention: 인코더에서 이루어짐. query, key, value의 출처가 동일
- masked self attention: 디코더에서 이루어짐. query, key, value의 출처가 동일
- encoder-decoder attetion: 디코더에서 이루어짐. query는 디코더 벡터, key와 value는 인코더 벡터

### self-attention

![transformer11.png](https://wikidocs.net/images/page/31379/transformer11.PNG)

- 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터를 사용하지 않고, 각 단어 벡터들로부터 Q, K, V 벡터를 얻음. ($d_{model}$보다 작은 차원)
- 차원: $d_{model}/num\,heads$ (논문에서는 num_heads를 8로 설정)
- 가중치 행렬을 곱함으로 완성, 가중치 행렬은 $d_{model} \times (d_{model}/num\,heads)$의 크기를 가짐.

이후의 과정은 기존의 attention 매커니즘과 동일

![img1.daumcdn.png](https://velog.velcdn.com/images%2Fjus6886%2Fpost%2Ffaf954d9-d150-458a-a793-d7521d5f4c4f%2Fimage.png)

![img1.daumcdn.png](https://velog.velcdn.com/images%2Fjus6886%2Fpost%2F4c5825d4-858d-4110-a54e-812721e947ff%2Fimage.png)

### multi-head attention

![차원.png](https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png)

![수식.png](https://i0.wp.com/www.blopig.com/blog/wp-content/uploads/2022/10/image-1.png?resize=624%2C110&ssl=1)

한 번의 attention 수행보다 여러 번의 attention을 병렬로 사용하는 것이 더 효과적이라고 판단.

→ $d_{model} / num\,heads$의 차원을 가지는 Q, K, V에 대해서 $num\,heads$개의 병렬 attention 수행

(이때, 각각의 attention 값 행렬을 attention head라고 부르며, 가중치 행렬 $W^Q$, $W^K$, $W^V$의 값은 attention head마다 전부 다름) ; 다양한 시각에서 정보 수집 가능(각 attention head는 전부 다른 시각에서 바라봄)

→ 병렬 attention 수행 후 모든 attention head를 concatenate.($(seq\,len, d_{model})$ 크기의 행렬 생성)

→ 생성한 행렬에 또 다른 가중치 행렬 $W^0$를 곱함

⇒ multi-head attention의 최종 결과

encoder의 첫 번째 서브층에서 이 과정을 마치고 난 후, encoder의 input 행렬의 크기가 그대로 유지되고 있으며, 두 번쨰 서브층에서도 신경망을 지난 후 행렬의 크기는 계속 유지되어야 함.

(인코더에서의 입력의 크기가 출력에서도 동일하게 유지되어야 다음 인코더에서도 입력이 가능)

### masked attention

![3-Figure2-1.png](https://user-images.githubusercontent.com/89114114/154040919-19170f4f-c1b9-4b95-aed7-e196a2782b55.png)

- 기존의 encoder-decoder 구조의 모델: 순차적으로 입력값을 전달받음 → $t+1$시점의 예측을 위해 $t$시점까지의 데이터만 사용할 수 있음.
- transformer: 전체 입력값을 전달받음 → 과거 시점의 입력값을 예측할 때 미래 시점의 입력값 참고 가능

⇒ 이를 방지하고자 look-ahead mask 사용, look-ahead mask를 이용하는 attention을 masked attention

![img.png](https://www.researchgate.net/profile/Tao-Shen-14/publication/324182063/figure/fig1/AS:611538339627009@1522813510409/Masked-self-attention-mechanism-fij-denotes-f-xi-xj-in-Eq9.png)

Attention Scores의 $(i, j)$는 i번째 query와 j번째 (key, value) 사이의 유사도를 의미함. → 이 matrix의 upper triangle matrix는 미래 시점의 입력값과의 유사도

⇒ attention value를 계산할 때 $i<j$인 요소는 고려하지 않음.

$-\infty$로 변경(look-ahead mask) 후 softmax를 취해 해당 요소들의 attention weight를 0으로 만듦

## layer normalization

### vs batch normalization

신경망의 각 layer마다 input의 분포가 달라지는 ‘covariate shift’

→ 이를 해소하고자 신경망의 각 layer에 들어가는 input을 batch 단위의 평균과 분산으로 normalization하여 학습을 효율적으로 만듦.

$$
\mu_{i} = \frac{1}{M}\sum_{m=1}^{N} x_{m,k}
$$

$$
\sigma_{i}^2 = \frac{1}{M}\sum_{m=1}^{K} (x_{m,k} - \mu_{i})^2
$$

batch 안의 모든 sample에 대해 k번째 feature의 평균과 분산을 구함.

- 단점
    - mini-batch 크기에 의존적
    - recurrent 기반 모델에 적용이 어려움(입력값이 기본적으로 sequence이기 때문)

⇒ layer normalization 도입

![images_stapers_post_4c5ca769-3cbc-4b32-a51d-0b626b6c49f0_LM.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdpdO6M%2FbtreEJEssEE%2Fswkx4xx7XyDGnKas5fAqN1%2Fimg.png)

$$
\mu_{i} = \frac{1}{K}\sum_{k=1}^{K} x_{i,k}
$$

$$
\sigma_{i}^2 = \frac{1}{K}\sum_{k=1}^{K} (x_{i,k} - \mu_{i})^2
$$

하나의 sample $x_{i}$의 모든 feature (1 … K)의 평균과 분산을 구함. → input을 기준으로 계산

![images_stapers_post_92fecd7a-0f93-4bf0-8dcb-f119c9d7b102_LM_ex.png](https://tensorflowkorea.files.wordpress.com/2016/07/layer-normlization.png?w=625)

- 데이터마다 각각 다른 normalization term $(\mu, \sigma)$를 가짐
- mini-batch 크기에 영향을 받지 않음 (size = 1이어도 작동)
- 서로 다른 길이를 갖는 sequence가 batch 단위의 입력으로 들어와도 적용 가능

일반적인 RNN 모델에서는 많은 time-step을 거칠 때, gradient가 explode 또는 vanish하는 현상이 발생하지만, LM적용 시 layer의 input의 scale에 영향을 받지 않으므로 안정적인 학습 진행.

## residual connection

비선형 함수의 결과에 기존 input을 더해줌.

(ensemble과 같은 효과, vanishing gradient 문제에도 도움이 되어 더 깊은 신경망 학습 가능)

![images_stapers_post_4d2d860a-9112-431e-99bc-c6093349e33c_rescon.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdBesUR%2FbtqVyUjjEyV%2FgCqE4ukhiXR2J0yw70xpOK%2Fimg.png)

- 예를 들어, 단순히 $f(x)$일 때 층이 깊어질 경우 gradient가 사라질 수 있지만, $f(x) + x$ 구조에서는 최소한 1의 gradient가 보장되기 때문에 안정적인 학습이 가능
- transformer의 encoder구조에서는 embedding vector에 positional encoding vector를 더해주는 방식

$y_l = h(x_l) + F(x_l, W_l)$

$x_{l+1} = f(y_l)$이

이때, $h(x_l) = x_l$ (identity mapping)

특정 위치에서의 $x_L$을 다음과 같이 $x_l$과 residual 함수의 합으로 나타낼 수 있음.

$x_2 = x_1 + F(x_1, W_1)$

$x_3 = x_2 + F(x_2, W_2) = x_1 + F(x_1, W_1) + F(x_2, W_2)$

$x_L = x_l + \sum_{i=1}^{L-1} F(x_i, W_i)$

이를 미분하면,

$\frac{\sigma\epsilon}{\sigma x_l} = \frac{\sigma\epsilon}{\sigma x_L}\frac{\sigma x_L}{\sigma x_l} = \frac{\sigma\epsilon}{\sigma x_L} (1+\frac{\sigma}{\sigma x_l} \sum_{i=1}^{L-1} F(x_i, W_i))$

- $\frac{\sigma\epsilon}{\sigma x_L}$은 상위 layer의 gradient 값이 변하지 않고 그대로 하위 layer에 전달되는 것을 보여줌.
- ⇒ layer를 거칠수록 gradient가 사라지는 vanishing gradient 문제 완화
- forward path, backward path를 간단하게 표현 가능