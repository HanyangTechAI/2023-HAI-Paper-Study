# 이현우

생성일: 2023년 3월 27일 오후 2:54

# HAI 논문 스터디 2주차

Introduction to Transformer

## Positional encoding

$$
A=[a_0, a_1, a_2,\ldots,a_n] \\
PE(pos,2i)=sin(pos/10000^{2i)/dmodel}) \\
PE(pos,2i+1)=cos(pos/10000^{2i/dmodel})
$$

- transformer에게 토큰의 위치 정보는 중요(예: not의 위치에 따라 문장의 뜻이 완전히 달라짐)
- 그러기에 위치의 Representation을 벡터로 표현하여 token embedding에 더해줌
- Positional Encoding은 여러 특징을 지니고 있어야 함
    - 각 위치의 값은 결정론으로 계산되며 유일해야 한다
    - 데이터에 관계없이 각 위치의 PE 값은 동일해야 한다
    - 데이터의 길이와 상관없이 적용 가능해야 한다
    - 간격이 동일하다면 위치와 관계없이 거리가 동일해야 한다.
- sin, cos는 주기함수이므로 데이터 길이에 대한 제한이 없음
- sin, cos 함수는 위치에 따라 형태가 다르므로 각 위치의 값이 유일하고 구분 가능

### Simple indexing

- 각 위치의 index를 PE 값으로 사용
- 입력 토큰이 길어질수록 PE 값 역시 커져 학습에 문제를 겪을 가능성이 높음

### Normalize simple indexing

- 앞선 문제를 해결하기 위해서 길이로 나누어 [0, 1] 값으로 normalize
- 학습과 실행에서 여러 문제가 발생할 수 있음
예)
단순히 입력의 길이를 기준으로 할 시 길이가 10인 데이터의 4번째와 길이가 20인 데이터의 8번째 PE가 같음
최대 길이를 기준으로 할 시 그 이상의 PE는 1을 초과

### Using binary

- simple indexing의 결과를 2진수를 이용해 표현
- 앞서 말한 4번 조건을 만족하지 못함
    
    
    | 위치 | 인덱스 | Binary Indexing |
    | --- | --- | --- |
    | a1 | 1 | [0,0,0,1] |
    | a2 | 2 | [0,0,1,0] |
    | a3 | 3 | [0,0,1,1] |
    | a4 | 4 | [0,1,0,0] |
    | a5 | 5 | [0,1,0,1] |

## Self attention

- query, key-value의 input이 같음
- 이후 모든 연산은 일반적인 attention과 동일

## Multi-head attention

- 여러 attention을 병렬로 실행→ensemble과 비슷 효과
- 각각의 attention 이후 concat
- 마지막에 o를 통과

## Masked attention

- 디코더에서는 다음 단어를 예측하는 task. 그렇기에 안보고 예측해야 함
- target 이후를 masking 처리
- 주로 -inf를 줌으로써 softmax에서 0이 되어 attention 되지 않게 함

## Layer Normalization

- 각 레이어의 출력을 평균과 분산으로 정규화
- 각 레이어의 입력 분포가 일정하게 유지되어 소실과 폭발 방지

## Residual connection

$$
\mathcal x_{l+1}=FF(Attn(x_l)+x_l)+x_l
$$

- transformers는 깊은 신경망이기 때문의 학습 과정에서 입력 데이터의 일부 특성을 잊어버릴 수 있음
- 계산 결과에 입력 x를 더하는 방식으로 작동
- 여러 장점을 지님
    - 역전파 과정에서 기울기가 사라지는 문제 완화
    - 정보가 사라지지 않고 유지되도록 함