# 김도겸

생성일: March 27, 2023 2:54 PM

### Positional Encoding

Positional Encoding is a technique used in NLP to encode sequence order information into the input data of a model. It involves adding a fixed vector to each input sequence, which encodes its position in the overall sequence. The most commonly used method is sinusoidal encoding, where each element in the positional encoding vector is a sinusoidal function of the position index. The use of positional encoding improves the performance of NLP models on sequence-based tasks, especially for longer input sequences.

### Multi Head Attention

Multi Head Attention is a mechanism used in Neural Networks, particularly in Transformer models for natural language processing tasks. It allows the model to attend to multiple parts of the input sequence in parallel, by computing multiple sets of queries, keys, and values through projecting the input sequence into multiple subspaces. These subspaces enable the model to attend to different aspects of the input sequence simultaneously. The use of multiple heads of attention is effective in capturing complex dependencies in the input sequence, and is now a key component of many state-of-the-art NLP models.

### Self Attention

Self-attention is an attention mechanism used in neural networks for natural language processing tasks. It enables a model to weigh the importance of different parts of the input sequence based on their relevance to the current element being processed. Self-attention computes a weighted sum of the input sequence for each element, where the weights are determined based on the similarity between the element being processed and all other elements in the sequence. The similarity is computed using a dot product between the vector representations of each element. The resulting attended input sequence is then used to compute the output for that element.

### Masked Attention

Masked Attention is applied during the self-attention step of the network and masks certain parts of the input sequence to focus only on relevant information for the current position. This technique is used to predict the next word in a sequence based on the previous words, and it makes it possible to use the model for tasks such as language generation.

### Transformer Architecture

The Transformer Architecture is a neural network used for NLP tasks such as language translation, modeling and text generation. It consists of an Encoder and a Decoder, and uses Self Attention and Multi-Head Attention mechanisms to attend to different parts of the input sequence when processing each individual element. It also uses Positional Encoding to add positional information to the input sequence.