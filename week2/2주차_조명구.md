# **Positional Encoding**

![Transformer](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9f5f79b0-fb3b-4d15-afbe-e0fbb2b01553%2FUntitled.png&blockId=073a47df-4ad9-4a68-b396-56e5d6210eb5)
>Positional Encoding in Transformer

<br/>


## **1. Input Embedding**
![input embedding](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7cb7fb50-3475-48ae-8a46-41d83476755f%2FUntitled.png&blockId=553324d0-5e79-479f-b7b8-411036e1eb76)
1. input에 데이터 입력
2. 각각 단어를 인덱스 값에 매칭
3. 인덱스 값들을 Input Embedding에 전달
   
   <br/>

   
![input embedding2](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F214425a5-db68-4ba2-a191-8cb518bc9ffa%2FUntitled.png&blockId=c03dab38-9221-42f8-8da9-ef89049054ba)
![input embedding3](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc7d5f597-399a-4084-ab8f-1bd8b340892e%2FUntitled.gif&blockId=c3f57ec7-a058-4179-bc78-4ec07183786f)
1. car, this와 같이 문맥상 의미가 같은 단어들은 임베딩 벡터값이 가까워진다. (두 단어가 공유하는 피처값이 존재하기 때문)

<br/>

## **~~2. Trasformer: Sequential? Parallel?~~**

## **3. Positional Encoding**
> 단어의 위치정보를 받아들이는게 중요하다

![positional encoding example](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F134ab235-80b2-4fed-974c-cd0f79aa7904%2FUntitled.png&blockId=0f049850-ca51-4806-8583-59638dcfd031)

- 이처럼 not의 위치에 따라 문장의 뜻이 달라진다.

![positional encoding example2](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F838ffade-625f-40b7-99ee-f1b3ae2709ac%2FUntitled.png&blockId=95ecc069-ec2b-4328-8a21-a6a88e9df021)
- 그림과 같이 각각 단어 벡터에 위치정보를 더해줘야 한다.
- 위치정보는 단어 벡터 값에 비해 매우 작아야 한다.
- 단어 벡터에 담긴 정보가 왜곡될 수 있기 때문이다.

> 따라서 크기가 1보다 작고 문장이 길어져도 값의 차이가 유의미한 `sine, cosine` 함수를 활용한다.
- `sine, cosine` 함수가 주기함수인 문제를 해결하기 위해 주기가 다른 `sine, cosine` 함수를 여러 개 활용한다.

![sine, cosine 함수 활용 예시](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F69ff2718-58c7-4107-ac7c-85fb2ce71e94%2Fcut1.gif&blockId=13b5a2b4-2938-4ba3-97a0-0e836cb1d704)

![Positional Encoding 함수](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe0156b7d-adee-4517-899e-2603e5c0e495%2FUntitled.png&blockId=2bc1ee3d-18f5-44bd-a930-82cb98138eb2)

- pos = position / i = 차원
- 서로 다른 벡터 정보 간의 위치 차이를 나타내는 함수를 수식화

<br/>



## **4. Input Embedding과 Postional Embedding 간의 연산**
### **4-1. Concatenate VS Summation**
> **Concatenate**
![Concatenate_example](https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F5a3a218b-5049-4a70-91f5-de5af7ef9126%2FUntitled.png&blockId=f5fb25b4-09eb-44b7-8ab8-db9004e2b906)

- Concatenate 기법은 정보가 섞이지 않고 정확하지만, 비용이 많이 든다.

- Summation 기법이 위에서 단어 벡터 + 위치 정보를 계산한 기법
- 정보가 섞일 수 있지만, 비용이 줄어든다.

# **Self Attention**

> `Self Attention`은 쿼리(query), 키(key), 밸류(value) 3개 요소 사이의 문맥적 관계성을 추출하는 과정입니다.

$$ Attention(Q,K,V)=softmax(QK^⊤/\sqrt d_{K})V $$

- Definition of Self Attention
- softmax: 어떤 값이든 0 ~ 1 사이 값으로 변환해주는 함수
- 어차피 softmax 함수 사용할거면 키 차원수의 제곱근으로 왜 나눠줌?
<br/>
<br/>
- `Attention Score`(출력값) 행렬의 (i, j)번째 요소 
- - i번째 입력값 (Query)와 j번째 입력값(Key, Value) 사이의 유사도
<br/>
<br/>


# **Multi Head Attention**

> `Self Attention`을 여러 번 수행하는 것.
> <br/>
> 하나의 `Task`를 여러 `Head`가 각각 수행함.

![Multi_Head_Attention](https://i.imgur.com/L3Qr7Nc.png)

- 최종 결과: `입력 단어 수  × 목표 차원수`

>-  `Multi-Head Attention`은 각각의 동일한 query, key, value vector에 관해 동시에 병렬적으로 `attention`을 구하는 방법
> <br/>
> - `Head`를 여러 개 둠으로써 한 단어(query)에 대한 정보를 여러 가지 관점에서 추출할 수 있다.
> 
<br/>
<br/>

# **Masked Attention**

- 기존 Encoder-Decoder 구조의 모델들은 순차적으로 입력값을 전달받았다.
- 따라서, t+1 시점의 데이터 예측을 위해서는 t 시점까지의 데이터만 활용할 수 있었다.
- `Transformer`는 전체 입력값을 전달받기 때문에, 과거 시점 데이터를 예측할 때 미래 시점 데이터를 참고할 수 있다는 문제가 있다.
- 이러한 문제를 방지하기 위한 `Look-ahead Mask` 기법을 사용

![masked attention](https://blog.kakaocdn.net/dn/bzROYr/btrBOuETpui/FWKxr9K7EEWLFpeVy2nTg0/img.png)
<br/>
<br/>
# **Layer Normalization**
> ## **Batch Normalization VS Layer Normalization**
<br/>

![BN vs LN](https://velog.velcdn.com/images%2Fstapers%2Fpost%2F4c5ca769-3cbc-4b32-a51d-0b626b6c49f0%2FLM.png)
<br/>
**BM이란?**
- 신경망의 각 layer에 들어가는 input을 batch 단위에서 평균, 분산을 이용한 정규화를 통해 효율적으로 학습하는 방법
- `Mini-batch` 크기에 의존적이다.
- `Recurrent` 기반 모델에 적용이 어렵다.
<br/>
> BM과 다르게 LM은 `Layer` 단위에서 `Noramalization`한다.

1. Data마다 다른 noramalization term을 가진다.
2. `Mini-batch` 크기에 영향을 받지 않는다(size=1이어도 가동).
3. 1에 의해 서로 다른 길이의 `sequence` 가 `batch` 단위의 입력으로 들어오는 경우에도 적용된다.
4. `Layer`의 `input`이나 `Scale`에 영향을 받지 않기 때문에 안정적인 학습이 가능하다. 
<br/>
<br/>
# **Residual Connection**

![residual_connection](https://velog.velcdn.com/images%2Fstapers%2Fpost%2F4d2d860a-9112-431e-99bc-c6093349e33c%2Frescon.png)

$$ f(x) + x $$
- 비선형함수 + input
- 최소한 1의 gradient가 보장되기 때문에 안정적인 학습 가능

<br/>
<br/>
